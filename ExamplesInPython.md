## Spark â€“ Lightning Fast Analytics Across a cluster

Python Tutorial 
Chapters 1-5 should be sufficient.
https://docs.python.org/2.7/tutorial/index.html

#### 001_preflight_check

Start up a spark shell in scala

```sh
./bin/pyspark
```
If Spark gives Networking error, use the following to force spark to use the local network.
```sh
SPARK_LOCAL_IP=127.0.0.1 ./bin/pyspark
```
```python
sc
sc.master
```

```python
data = xrange(1, 10000)
distData = sc.parallelize(data)
distData.filter(lambda x: x < 10).collect()
```

#### 002_spark_deconstructed

```python
lines = sc.textFile("/home/pbashyal/data/web.log.gz")

# Transformations
errors = lines.filter(lambda l: "ERROR" in l)
messages = errors.map(lambda e: e.split("\t")).map(lambda r: r[1])
messages.cache() // Keep this RDD in memory if you can

# Actions
messages.filter(lambda l: "php" in l).count()
messages.filter(lambda l: "mysql" in l).count()
```
The driver is where your spark shell or your application is running. The driver knows how to talk to the cluster.
You can look at the lineage graph or operator graph by

```python
messages.toDebugString()
```

#### 003_a_brief_history

Latency increases as number of different systems need to work together and data needs to be moved around(wire tax). You want a common framework that can run these kinds of task, not just map/reduce.
Uses native runtime systems for running each language. Uses native Python and Py4J gateway.

#### 004_simple_spark_apps

*wordcount.python*
```python
f = sc.textFile("README.md")
# Transform 
# For each line split it into keywords, and map each keyword to a tuple with word with count 1
# Flatmap flattens the result so that you only have a list of keywords, rather than lists of lists
# reduceByKey, reduces all the data in each node by Key using the supplied lambda/function
# from operator import add
wc = f.flatMap(lambda l: l.split(" ")).map(lambda word: (word, 1)).reduceByKey(add)

# saveAsTextFile cause the lineage graph to be evaluated, take the results in parallel and save the results as partitioned file
wc.saveAsTextFile("wc_out") // will output  part-00000 and  part-00001 in wc_out directory
```
**Joins**
```python

# Read reg.tsv, split on tab character and create a (key, value) tuple with uuid as key
# and Register class as value
reg = sc.textFile("reg.tsv").map(lambda l: l.split("\t")).map(lambda r: (r[1], (r[0], int(r[2]))))

# Read clk.tsv, split on the tab character and create a (key, value) with uuid as key
# and Click class as a value
clk = sc.textFile("clk.tsv").map(lambda l: l.split("\t")).map(lambda r: (r[1], (r[0], int(r[2]))))

# Join by the common element: uuid
reg.join(clk).collect()
```

**Exercise**
```python
from operator import add
# Read lines from text files
readme = sc.textFile("README.md")
changes = sc.textFile("CHANGES.txt")
# Filter 'Spark' lines and perform word counts on those lines
readmeSparkWords = readme.filter(lambda l: l.find("Spark") >= 0).flatMap(lambda l: l.split(" ")).map(lambda word: (word, 1)).reduceByKey(add)
changesSparkWords = changes.filter(lambda l: l.find("Spark") >= 0).flatMap(lambda l: l.split(" ")).map(lambda word: (word, 1)).reduceByKey(add)

# Perform join
readmeSparkWords.join(changesSparkWords).map(lambda kv: (kv[0], (kv[1][0] + kv[1][1]))).collect()
```
#### 005_spark_essentials
RDD - Distributed Data Set 
Creating RDDs from native list
```python
data = [1, 2, 3, 4, 5]
distData = sc.parallelize(data)
```
Using Hadoop Connectors

```python
distData = sc.textFile("README.md")
```

Trasformations
```python
distFile = sc.textFile("README.md")
# Difference between map and flatMap
distFile.map(lambda l: l.split(" ")).collect()
distFile.flatMap(lambda l: l.split(" ")).collect()
```
#### 007_unifying_the_pieces_spark_sql
```python
# sqlContext is initialized as part of the spark-shell

from pyspark.sql import SQLContext, Row

lines = sc.textFile("examples/src/main/resources/people.txt")
parts = lines.map(lambda l: l.split(","))
people = parts.map(lambda p: Row(name = p[0], age = int(p[1])))

peopleTable = sqlContext.createDataFrame(people)
peopleTable.registerTempTable("people")

teenagers = sqlContext.sql("SELECT name FROM people WHERE age >= 13 AND age <= 19")

# Show the query plan
teenagers.explain()

# Print the results
teenagers.collect()

# Save as parquet file
peopleTable.saveAsParquetFile("people.parquet")

// Read a parquet file
parquetFile = sqlContext.read.parquet("people.parquet")
parquetFile.schema
parquetFile.registerTempTable("parquetFile")
teenagers = sqlContext.sql("SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19")
teenagers.explain()
```

Using Spark SQL DSL
```python
from pyspark.sql import SQLContext, Row

lines = sc.textFile("examples/src/main/resources/people.txt")
parts = lines.map(lambda l: l.split(","))
people = parts.map(lambda p: Row(name = p[0], age = int(p[1])))

peopleTable = sqlContext.createDataFrame(people)
peopleTable.registerTempTable("people")

teenagers = peopleTable.where("age >= 13").where("age <= 19").select("name")
teenagers.collect()
```

IPython Notebook
```sh
IPYTHON_OPTS="notebook" ./bin/pyspark
```
```python
from pyspark.sql import SQLContext, Row
sqlContext = SQLContext(sc) 

lines = sc.textFile("examples/src/main/resources/people.txt")
parts = lines.map(lambda l: l.split(","))
people = parts.map(lambda p: Row(name = p[0], age = int(p[1])))

people.collect()

peopleTable = sqlContext.createDataFrame(people)
peopleTable.registerTempTable("people")

teenagers = sqlContext.sql("SELECT name FROM people WHERE age >= 13 AND age <= 19")

teennames = teenagers.map(lambda p: "Name:" + p.name)
teennames.collect()
```
#### 008_unifying_the_pieces_spark_streaming
```sh
nc -lk 9999
```
Submit wordcount to the cluster
```sh
# Python
bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999
```
